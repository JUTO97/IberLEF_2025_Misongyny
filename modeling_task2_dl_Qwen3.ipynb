{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d5fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch, os, pandas as pd, numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, tokenizer_utils\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments, DataCollatorForLanguageModeling, DataCollatorWithPadding, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from typing import List, Union, Any, Dict\n",
    "from huggingface_hub import create_repo, upload_folder, notebook_login, login\n",
    "from utils_dl import set_global_seed\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4e878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper funct. needed as this function doesn't like it when the lm_head has its size changed\n",
    "def do_nothing(*args, **kwargs):\n",
    "    pass\n",
    "    \n",
    "tokenizer_utils.fix_untrained_tokens = do_nothing\n",
    "\n",
    "SEED=42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SUBTASK2_PATH = 'new_data\\subtask2'\n",
    "\n",
    "set_global_seed(SEED)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "language = 'spa'\n",
    "model_type = 'dl'\n",
    "stemming = False\n",
    "lemmatization = False\n",
    "remove_duplicates = False\n",
    "cased = True \n",
    "description = False\n",
    "\n",
    "desc = ''\n",
    "if description:\n",
    "    desc = '_with_description'\n",
    "\n",
    "data_config = f\"lang_{language}_model_{model_type}_stem_{stemming}_lem_{lemmatization}_dup_{remove_duplicates}_cased_{cased}{desc}\"\n",
    "file_name = 'subtask2_all_aug' \n",
    "db_file_name = f\"{file_name}_{data_config}.csv\"\n",
    "file_path = os.path.join(SUBTASK2_PATH, db_file_name)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    full_data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    print(\"File found:\")\n",
    "    print(full_data.info())\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File not found at {file_path}\")\n",
    "\n",
    "original_data = full_data[full_data['is_augmented'] != True].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5a83d-3919-45c1-955b-15644dac9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_aug_nr = (full_data['is_augmented'] == True) & (full_data['label'] == 'NR')\n",
    "cond_aug_s = (full_data['is_augmented'] == True) & (full_data['label'] == 'S')\n",
    "\n",
    "\n",
    "full_data = full_data[~(cond_aug_nr | cond_aug_s)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column  = \"lyrics_clean\"\n",
    "label_column = \"label\"\n",
    "group_column = \"id\"             # all augmented variants share this id\n",
    "aug_col      = \"is_augmented\"   # bool\n",
    "final_training = False \n",
    "val_split_size = 0.1 if final_training else 0.2\n",
    "\n",
    "if full_data[label_column].dtype == object:\n",
    "    unique_labels = sorted(full_data[label_column].unique())\n",
    "    label2id      = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "else:\n",
    "    unique_labels = sorted(full_data[label_column].unique())\n",
    "    label2id      = {int(lbl): int(lbl) for lbl in unique_labels}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "full_data[label_column] = full_data[label_column].map(label2id)\n",
    "original_data[label_column] = original_data[label_column].map(label2id)\n",
    "\n",
    "full_data = full_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "original_data = original_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "n_splits = int(1 / val_split_size)\n",
    "sgkf = StratifiedGroupKFold(\n",
    "    n_splits=n_splits,\n",
    "    shuffle=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "train_val_idx, test_idx = next(\n",
    "    sgkf.split(\n",
    "        original_data,\n",
    "        original_data[label_column],   # stratify on true labels\n",
    "        original_data[group_column]    # keep groups intact\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "train_val_df = original_data.iloc[train_val_idx].reset_index(drop=True)\n",
    "test_df      = original_data.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "if not final_training:\n",
    "    sgkf_val = StratifiedGroupKFold(\n",
    "        n_splits=8,\n",
    "        shuffle=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    trn_idx, val_idx = next(\n",
    "        sgkf_val.split(\n",
    "            train_val_df,\n",
    "            train_val_df[label_column],\n",
    "            train_val_df[group_column]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_df = train_val_df.iloc[trn_idx].reset_index(drop=True)\n",
    "    val_df   = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
    "    # print(f\"train={len(train_df)},  val={len(val_df)},  test={len(test_df)}\")\n",
    "\n",
    "else:\n",
    "    print('Dataset for final model training')\n",
    "    train_df = train_val_df.copy()\n",
    "    val_df   = test_df.copy()\n",
    "    # print(f\"train={len(train_df)},  val={len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8b4a1-7342-4fcf-8147-2d5b7053eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df[['id']], full_data, on = \"id\", how = \"inner\").copy()\n",
    "train_df = train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Train / Val / Test**\n",
    "save_datasets = True\n",
    "if not final_training: \n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    val_dataset  = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    test_dataset  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "    \n",
    "    ds = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    if save_datasets:\n",
    "        splits_folder_path = f\"{file_name}_{data_config}\"\n",
    "        if not os.path.exists(splits_folder_path):\n",
    "            os.makedirs(splits_folder_path)\n",
    "            \n",
    "        train_df.to_csv(os.path.join(splits_folder_path, 'train.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(train_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'train_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        val_df.to_csv(os.path.join(splits_folder_path, 'val.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(val_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'val_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        test_df.to_csv(os.path.join(splits_folder_path, 'test.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(test_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'test_ids.csv'), index=False, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "else: # **Train / Val**\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset  = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    ds = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "    \n",
    "    if save_datasets:\n",
    "        splits_folder_path = f\"{file_name}_{data_config}_competition\"\n",
    "        if not os.path.exists(splits_folder_path):\n",
    "            os.makedirs(splits_folder_path)\n",
    "            \n",
    "        train_df.to_csv(os.path.join(splits_folder_path, 'train_competition.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(train_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'train_competition_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        val_df.to_csv(os.path.join(splits_folder_path, 'val_competition.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(val_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'val_competition_ids.csv'), index=False, encoding='utf-8')\n",
    "\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60b94b",
   "metadata": {},
   "source": [
    "## Instruction Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f120579",
   "metadata": {},
   "source": [
    "**Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "### Instruction: \n",
    "Classify the misogyny subtype of the following lyric.\n",
    "Categories:\n",
    "- S: describe or suggest sexual acts, sexual language, or insinuations\n",
    "- V: physical or verbal aggression, threats, or violent actions\n",
    "- H: hoffensive or discriminatory language, expressions of contempt, or hostility towards a group or individual\n",
    "- NR  : none of the above\n",
    "Return only one label from: S, V, H, NR\n",
    "### Input:\n",
    "{lyrics_clean}\n",
    "### Response:\n",
    "{label}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af4379-a286-4ccf-9720-c0cfec0743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lora and model downloading parameters\n",
    "lora_parameters = {\n",
    "    'lora_r' : 16,\n",
    "    'target_modules' : [\"lm_head\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    'lora_alpha' : 16,\n",
    "    'lora_dropout' : 0, # available also another implementations, but 0 is optimized in unsloth\n",
    "    'lora_bias' : \"none\",\n",
    "    'lora_use_gradient_checkpointing' : \"unsloth\",\n",
    "    'lora_random_state' : 3407,\n",
    "    'lora_use_rslora' : True,\n",
    "    # 'lora_loftq_config' : None\n",
    "}\n",
    "model_parameters = {\n",
    "    'model_name' : 'unsloth/Qwen3-14B-Base-unsloth-bnb-4bit',\n",
    "    'model_max_seq_length' : 4096,\n",
    "    'model_dtype' : None ,\n",
    "    'model_load_in_4bit' : True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_parameters['model_name'],\n",
    "    max_seq_length = model_parameters['model_max_seq_length'],\n",
    "    dtype = model_parameters['model_dtype'],\n",
    "    load_in_4bit = model_parameters['model_load_in_4bit'],\n",
    "    force_download=True, \n",
    ")\n",
    "# Replace lm_head with 4 token head torch.Size([4, 5120])\n",
    "label_token_ids: Dict[int,int] = {}\n",
    "for i, spaced_lbl in id2label.items():\n",
    "    toks = tokenizer.encode(spaced_lbl, add_special_tokens=False)\n",
    "    assert len(toks)==1, f\"{spaced_lbl!r} is still {toks}\"\n",
    "    label_token_ids[toks[0]] = i\n",
    "\n",
    "orig_head  = model.lm_head                   # [vocab × hidden]\n",
    "old_shape = model.lm_head.weight.shape\n",
    "old_size = old_shape[0]\n",
    "hidden_dim = orig_head.weight.shape[1]\n",
    "classifier = nn.Linear(hidden_dim, len(label2id), bias=False)\n",
    "\n",
    "# copy rows in class‐order:\n",
    "row_ids = [tok_id for tok_id, cls in sorted(label_token_ids.items(), key=lambda kv: kv[1])]\n",
    "classifier.weight.data = orig_head.weight.data[row_ids]\n",
    "model.lm_head = classifier      \n",
    "\n",
    "\n",
    "print(model.lm_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9aa84f-4c6d-42c3-8ace-d7c746f19c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_parameters['lora_r'],\n",
    "    target_modules = lora_parameters['target_modules'],\n",
    "    lora_alpha = lora_parameters['lora_alpha'],\n",
    "    lora_dropout = lora_parameters['lora_dropout'],\n",
    "    bias = lora_parameters['lora_bias'],\n",
    "    use_gradient_checkpointing = lora_parameters['lora_use_gradient_checkpointing'],\n",
    "    random_state = lora_parameters['lora_random_state'],\n",
    "    use_rslora = lora_parameters['lora_use_rslora'],  \n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a696439-1e92-4b1f-9a4f-68f17e6cfdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Datacollator for last token 4 class loss\n",
    "class DataCollatorForLastToken4Way(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        label_token_ids: Dict[int, int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_token_ids = label_token_ids or {}\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "        labels = batch[\"labels\"]  # [B, seq_len]\n",
    "        for i in range(labels.size(0)):\n",
    "            seq = labels[i]\n",
    "            last_idx = (seq != self.ignore_index).nonzero()[-1].item()\n",
    "            seq[:last_idx] = self.ignore_index\n",
    "            tok = seq[last_idx].item()\n",
    "            seq[last_idx] = self.label_token_ids.get(tok, self.ignore_index)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "collator = DataCollatorForLastToken4Way(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    ignore_index=-100,\n",
    "    label_token_ids=label_token_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3814b0-f211-43f5-838b-bbdd421e36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(examples):\n",
    "    texts = []\n",
    "    for lyric, lbl in zip(examples[\"lyrics_clean\"], examples[\"label\"]):\n",
    "        texts.append(\n",
    "            PROMPT.format(\n",
    "                lyrics_clean=lyric, \n",
    "                label=id2label[int(lbl)], \n",
    "            ) \n",
    "        )\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "def build_prompt_test(examples):\n",
    "    texts = []\n",
    "    for lyric, lbl in zip(examples[\"lyrics_clean\"], examples[\"label\"]):\n",
    "        texts.append(\n",
    "            PROMPT.format(\n",
    "                lyrics_clean=lyric, \n",
    "                label=\"\"\n",
    "            ) \n",
    "        )\n",
    "    return {\"text\": texts}\n",
    "\n",
    "ds_train = ds[\"train\"].map(\n",
    "    build_prompt,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_val = ds[\"val\"].map(\n",
    "    build_prompt_test,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "ds_test = ds[\"test\"].map(\n",
    "    build_prompt_test,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fb9c4",
   "metadata": {},
   "source": [
    "## Model Learning (Instruction Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2c1a5-55d6-4879-913a-183137852a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = {\n",
    "    'evaluation_strategy' : \"steps\",\n",
    "    'logging_strategy' : \"steps\",\n",
    "    'save_strategy' : \"epoch\",\n",
    "    'eval_steps' : 10,\n",
    "    'per_device_train_batch_size' : 4,\n",
    "    'evaluation_strategy' : \"steps\",\n",
    "    'logging_strategy' : \"steps\",\n",
    "    'save_strategy' : \"epoch\",\n",
    "    'gradient_accumulation_steps' : 2,\n",
    "    'warmup_steps' : 10,\n",
    "    'max_grad_norm':0.3,\n",
    "    'learning_rate' : 1e-4,\n",
    "    'fp16' : not is_bfloat16_supported(),\n",
    "    'bf16' : is_bfloat16_supported(),\n",
    "    'logging_steps' : 1,\n",
    "    'optim' : \"adamw_8bit\",\n",
    "    'weight_decay' : 0.001,\n",
    "    'lr_scheduler_type' : \"cosine\",\n",
    "    'seed' : 3407,\n",
    "    'output_dir' : \"outputs\",\n",
    "    'num_train_epochs' : 3,\n",
    "}\n",
    "\n",
    "params = {**lora_parameters, **model_parameters, **training_arguments}\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=ds_train,     \n",
    "    eval_dataset = ds_val,       \n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = model_parameters['model_max_seq_length'],\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        eval_strategy=training_arguments['evaluation_strategy'],\n",
    "        logging_strategy=training_arguments['logging_strategy'],\n",
    "        save_strategy=training_arguments['save_strategy'],\n",
    "        eval_steps=training_arguments['eval_steps'],\n",
    "        per_device_train_batch_size = training_arguments['per_device_train_batch_size'],\n",
    "        gradient_accumulation_steps = training_arguments['gradient_accumulation_steps'],\n",
    "        warmup_steps = training_arguments['warmup_steps'], \n",
    "        max_grad_norm = training_arguments['max_grad_norm'],\n",
    "        learning_rate = training_arguments['learning_rate'],\n",
    "        fp16 = training_arguments['fp16'],\n",
    "        bf16 = training_arguments['bf16'],\n",
    "        logging_steps = training_arguments['logging_steps'],\n",
    "        optim = training_arguments['optim'],\n",
    "        weight_decay = training_arguments['weight_decay'],\n",
    "        lr_scheduler_type = training_arguments['lr_scheduler_type'],\n",
    "        seed = training_arguments['seed'],\n",
    "        output_dir = training_arguments['output_dir'],\n",
    "        num_train_epochs=training_arguments['num_train_epochs'],\n",
    "        group_by_length = True, \n",
    "    ),\n",
    "    data_collator           = collator,\n",
    ")\n",
    "\n",
    "trainer.model.print_trainable_parameters()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bc61f-a512-44a7-9508-2aaa8070cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb9a4d-318d-49db-88c2-bde337c9b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(ds_val[\"text\"], truncation=True, padding=\"longest\",\n",
    "                max_length=4096, return_tensors=\"pt\", add_special_tokens=False)\n",
    "eval_ds = TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"],\n",
    "                       torch.tensor(ds_val[\"label\"], dtype=torch.long))\n",
    "loader  = DataLoader(eval_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for in_ids, attn, labs in loader:\n",
    "        in_ids, attn = in_ids.to(device), attn.to(device)\n",
    "        out = model(input_ids=in_ids, attention_mask=attn).logits\n",
    "        if out.ndim == 3:\n",
    "            out = out[:, -1, :]           # pick last‐token logits\n",
    "        probs = torch.softmax(out, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1).cpu().tolist()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a709e6-2eb0-4cf7-86fe-e1d18c745fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 69.23% (81/117)\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H     0.5714    0.5714    0.5714         7\n",
      "          NR     0.8776    0.7288    0.7963        59\n",
      "           S     0.5455    0.8571    0.6667        35\n",
      "           V     0.6667    0.2500    0.3636        16\n",
      "\n",
      "    accuracy                         0.6923       117\n",
      "   macro avg     0.6653    0.6018    0.5995       117\n",
      "weighted avg     0.7311    0.6923    0.6849       117\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 4  2  1  0]\n",
      " [ 1 43 13  2]\n",
      " [ 2  3 30  0]\n",
      " [ 0  1 11  4]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation accuracy: {accuracy*100:.2f}% ({sum(p==t for p,t in zip(all_preds, all_labels))}/{len(all_labels)})\")\n",
    "\n",
    "target_names = [id2label[i] for i in range(len(id2label))]\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"\\nConfusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089410c2",
   "metadata": {},
   "source": [
    "## Save Adapter to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d74c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = model_parameters['model_name'].split('/')[-1]\n",
    "REPO_ID = f\"repo_name\"\n",
    "\n",
    "model.save_pretrained(MODEL_NAME, safe_serialization=True)\n",
    "\n",
    "model.push_to_hub(\n",
    "    repo_id     = REPO_ID,   # creates repo if it doesn’t exist\n",
    "    token       = \"your_token\",                     # or rely on `huggingface-cli login`\n",
    "    commit_message = (\n",
    "        f\"Adapter. Classes: {len(id2label)}. \"\n",
    "        f\"Hyper‑parameters: {params}\"\n",
    "    ),\n",
    "    safe_serialization = True,        \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
