{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from huggingface_hub import create_repo, upload_folder, notebook_login, login\n",
    "from utils_dl import set_global_seed\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4e878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Using GPU: NVIDIA RTX A6000\n",
      "Pytorch version: 2.4.1+cu124\n"
     ]
    }
   ],
   "source": [
    "SEED=42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SUBTASK_2_PATH = \"new_data\\subtask2\"\n",
    "\n",
    "set_global_seed(SEED)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "print(\"Pytorch version:\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into HF account\n",
    "notebook_login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "language = 'spa'\n",
    "model_type = 'dl'\n",
    "stemming = False\n",
    "lemmatization = False\n",
    "remove_duplicates = False\n",
    "cased = True \n",
    "description = False\n",
    "\n",
    "desc = ''\n",
    "if description:\n",
    "    desc = '_with_description'\n",
    "\n",
    "data_config = f\"lang_{language}_model_{model_type}_stem_{stemming}_lem_{lemmatization}_dup_{remove_duplicates}_cased_{cased}{desc}\"\n",
    "file_name = 'subtask2_all_aug' \n",
    "db_file_name = f\"{file_name}_{data_config}.csv\"\n",
    "file_path = os.path.join(SUBTASK_2_PATH, db_file_name)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    full_data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    # print(\"File found:\")\n",
    "    # print(full_data.info())\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File not found at {file_path}\")\n",
    "\n",
    "# New DF without augmented rows\n",
    "original_data = full_data[full_data['is_augmented'] != True].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5a83d-3919-45c1-955b-15644dac9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_aug_nr = (full_data['is_augmented'] == True) & (full_data['label'] == 'NR')\n",
    "cond_aug_s = (full_data['is_augmented'] == True) & (full_data['label'] == 'S')\n",
    "\n",
    "full_data = full_data[~(cond_aug_nr | cond_aug_s)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column  = \"lyrics_clean\"\n",
    "label_column = \"label\"\n",
    "group_column = \"id\"             # all augmented variants share this id\n",
    "aug_col      = \"is_augmented\"   # bool\n",
    "final_training = False \n",
    "val_split_size = 0.1 if final_training else 0.2\n",
    "\n",
    "# Map labels to integers if needed\n",
    "if full_data[label_column].dtype == object:\n",
    "    unique_labels = sorted(full_data[label_column].unique())\n",
    "    label2id      = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "else:\n",
    "    unique_labels = sorted(full_data[label_column].unique())\n",
    "    label2id      = {int(lbl): int(lbl) for lbl in unique_labels}\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "full_data[label_column] = full_data[label_column].map(label2id)\n",
    "original_data[label_column] = original_data[label_column].map(label2id)\n",
    "\n",
    "# Shuffle before splitting\n",
    "full_data = full_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "original_data = original_data.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "n_splits = int(1 / val_split_size)\n",
    "sgkf = StratifiedGroupKFold(\n",
    "    n_splits=n_splits,\n",
    "    shuffle=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "train_val_idx, test_idx = next(\n",
    "    sgkf.split(\n",
    "        original_data,\n",
    "        original_data[label_column],   # stratify on true labels\n",
    "        original_data[group_column]    # keep groups intact\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "train_val_df = original_data.iloc[train_val_idx].reset_index(drop=True)\n",
    "test_df      = original_data.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "if not final_training:\n",
    "\n",
    "    sgkf_val = StratifiedGroupKFold(\n",
    "        n_splits=8, \n",
    "        shuffle=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    trn_idx, val_idx = next(\n",
    "        sgkf_val.split(\n",
    "            train_val_df,\n",
    "            train_val_df[label_column],\n",
    "            train_val_df[group_column]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_df = train_val_df.iloc[trn_idx].reset_index(drop=True)\n",
    "    val_df   = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
    "    # print(f\"train={len(train_df)},  val={len(val_df)},  test={len(test_df)}\")\n",
    "\n",
    "else:\n",
    "    print('Dataset for final model training')\n",
    "    train_df = train_val_df.copy()\n",
    "    val_df   = test_df.copy()\n",
    "    # print(f\"train={len(train_df)},  val={len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12d09f-406e-4648-a78a-dcb6ac157ea5",
   "metadata": {},
   "source": [
    "Now, only training set contains augmented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f22cb9-a985-4ebc-9ff9-6a2fa547f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df[['id']], full_data, on = \"id\", how = \"inner\").copy()\n",
    "train_df = train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Train / Val / Test**\n",
    "save_datasets = True\n",
    "if not final_training: \n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    val_dataset  = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    test_dataset  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "    \n",
    "    ds = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    if save_datasets:\n",
    "        splits_folder_path = f\"{file_name}_{data_config}\"\n",
    "        if not os.path.exists(splits_folder_path):\n",
    "            os.makedirs(splits_folder_path)\n",
    "            \n",
    "        train_df.to_csv(os.path.join(splits_folder_path, 'train.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(train_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'train_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        val_df.to_csv(os.path.join(splits_folder_path, 'val.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(val_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'val_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        test_df.to_csv(os.path.join(splits_folder_path, 'test.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(test_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'test_ids.csv'), index=False, encoding='utf-8')\n",
    "    \n",
    "    \n",
    "else: # **Train / Val**\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset  = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    ds = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset\n",
    "    })\n",
    "    \n",
    "    if save_datasets:\n",
    "        splits_folder_path = f\"{file_name}_{data_config}_competition\"\n",
    "        if not os.path.exists(splits_folder_path):\n",
    "            os.makedirs(splits_folder_path)\n",
    "            \n",
    "        train_df.to_csv(os.path.join(splits_folder_path, 'train_competition.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(train_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'train_competition_ids.csv'), index=False, encoding='utf-8')\n",
    "        \n",
    "        val_df.to_csv(os.path.join(splits_folder_path, 'val_competition.csv'), index=False, encoding='utf-8')\n",
    "        unique_ids_df = pd.DataFrame(val_df['id'].unique(), columns=['id'])\n",
    "        unique_ids_df.to_csv(os.path.join(splits_folder_path, 'val_competition_ids.csv'), index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28cfa0",
   "metadata": {},
   "source": [
    "## Instruction Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ab575",
   "metadata": {},
   "source": [
    "**Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "### Instruction: \n",
    "Classify the misogyny subtype of the following lyric.\n",
    "Categories:\n",
    "- S: describe or suggest sexual acts, sexual language, or insinuations\n",
    "- V: physical or verbal aggression, threats, or violent actions\n",
    "- H: offensive or discriminatory language, expressions of contempt, or hostility towards a group or individual\n",
    "- NR: none of the above\n",
    "The correct answer is one label from: S, V, H, NR\n",
    "\n",
    "### Input:\n",
    "{lyrics_clean}\n",
    "\n",
    "### Response:\n",
    "{label}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dedcb8-beed-4227-95c0-e5cc35f8f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = 'meta-llama/Llama-3.1-8B'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b58fea-2adb-4a62-b421-3b5020db9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prompt_train(examples):\n",
    "    texts = []\n",
    "    for lyric, lbl in zip(examples[\"lyrics_clean\"], examples[\"label\"]): \n",
    "        texts.append(\n",
    "            PROMPT.format(\n",
    "                lyrics_clean=lyric, \n",
    "                label=id2label[int(lbl)],\n",
    "                \n",
    "            ) \n",
    "        )\n",
    "    return {\"text\": texts}\n",
    "\n",
    "def build_prompt_val(examples):\n",
    "    texts = []\n",
    "    for lyric, lbl in zip(examples[\"lyrics_clean\"], examples[\"label\"]):\n",
    "\n",
    "        texts.append(\n",
    "            PROMPT.format(\n",
    "                lyrics_clean=lyric, \n",
    "                label=\"\",\n",
    "            ) \n",
    "        )\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Map that function over all splits, in batched mode\n",
    "train_data = ds['train'].map(\n",
    "    build_prompt_train,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_data = ds['val'].map(\n",
    "    build_prompt_val,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_data = ds['test'].map(\n",
    "    build_prompt_val,\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "      \"id\", \"lyrics\", \n",
    "      \"augmentation_type\", \n",
    "        \"is_augmented\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70122dda",
   "metadata": {},
   "source": [
    "## Model Learning (Instruction Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa44e22-89ff-4642-a21e-aed14232e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, tokenizer):\n",
    "    y_pred = []\n",
    "\n",
    "    # Use pipeline once outside the loop for efficiency\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=2,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        prompt = example[\"text\"]\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
    "        # Match prediction to valid category\n",
    "        for category in unique_labels:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(\"none\")  # fallback\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# y_pred = predict(eval_data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58402ef2-3692-4014-bd1b-a545200872a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, label_list):\n",
    "    # Create label to index mapping\n",
    "    labels = label_list\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    # check\n",
    "    assert -1 not in y_true_mapped, \"Unknown label in y_true\"\n",
    "    assert -1 not in y_pred_mapped, \"Unknown label in y_pred\"\n",
    "\n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'\\nOverall Accuracy: {accuracy:.3f}\\n')\n",
    "\n",
    "    # Per-label accuracy\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == idx]\n",
    "        if not label_indices:\n",
    "            print(f'No samples for label {label}')\n",
    "            continue\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {label_accuracy:.3f}')\n",
    "\n",
    "    # Classification report\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_true_mapped, y_pred_mapped, target_names=labels))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(confusion_matrix(y_true_mapped, y_pred_mapped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc42020-ad03-4f32-96fa-b57e856b4a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45800593-0eab-4c16-ad11-db30756faf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"llama-3.1-fine-tuned-model\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0, \n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=3,                       # number of training epochs\n",
    "    per_device_train_batch_size=2,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,                         \n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"none\",                         # report metrics to w&b\n",
    "    eval_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    eval_steps=0.2,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    \n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1526c90-8b0a-4b84-a873-c9dbd15de61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "y_true = [id2label[i] for i in test_data[\"label\"]]\n",
    "y_pred = predict(test_data, model, tokenizer)\n",
    "y_pred = list(map(lambda x: 'NR' if x == 'none' else x, y_pred))\n",
    "evaluate(y_true, y_pred, label_list=unique_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
